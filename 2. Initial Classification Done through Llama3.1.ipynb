{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb455f7d-3c79-4ab0-9f47-04e4a551e006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifications saved to classified_results.csv in the same directory as your Python file.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = 'test_data - Prompt.csv'  \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Filter out rows where the text column is empty or missing\n",
    "data = data[data['text'].notna()].reset_index(drop=True)\n",
    "\n",
    "# Define a function to create a detailed classification prompt\n",
    "def create_prompt(text):\n",
    "    return f\"\"\"\n",
    "You are an advanced AI model tasked with classifying text inputs into one of the following injection classes. Each class is defined with a clear description and purpose:\n",
    "\n",
    "1. Active Injection: Malicious prompts that are actively delivered to an LLM, such as sending emails containing harmful prompts. These prompts manipulate the LLM to execute malicious actions, leak sensitive data, or generate undesired outputs.\n",
    "\n",
    "2. Passive Injection: Malicious content embedded in external sources (e.g., webpages or databases) that the LLM might read. The LLM unknowingly processes this content, leading to misinformation or executing harmful actions.\n",
    "\n",
    "3. User-driven Injection: Innocent-looking prompts shared with users that cause malicious behavior when the user copies and pastes them into the LLM environment. These are often designed using social engineering techniques.\n",
    "\n",
    "4. Virtual Prompt Injection: Manipulations to the LLM’s instruction set or training data that make the model behave in unintended ways. The attacker embeds additional instructions to alter outputs, often introducing bias or unexpected behaviors.\n",
    "\n",
    "5. Double Character: Crafting prompts with similar-looking or combined characters to bypass LLM restrictions. These prompts exploit the LLM's inability to distinguish certain characters, tricking it into providing malicious outputs.\n",
    "\n",
    "6. Virtualization: Prompts designed to push the LLM into an unrestricted mode (e.g., \"developer\" mode or \"virtual machine\"). In this mode, the LLM can execute harmful or unauthorized commands.\n",
    "\n",
    "7. Obfuscation: Concealing malicious instructions using methods like encoding (e.g., Base64) or replacing characters with symbols. These instructions bypass the LLM’s filters and deliver harmful content.\n",
    "\n",
    "8. Payload Splitting: Splitting a malicious instruction into multiple parts that appear harmless when separate but execute harmful behavior when combined. For example, combining benign texts A and B into a malicious result A+B.\n",
    "\n",
    "9. Adversarial Suffix: Adding carefully crafted suffixes to a prompt to bypass LLM safeguards or trick the system into generating harmful outputs. These suffixes often alter the intended behavior of the model.\n",
    "\n",
    "10. Instruction Manipulation: Attempts to modify or reveal the LLM’s internal system instructions. This includes requests to expose system prompts or ignore default restrictions to produce malicious outputs.\n",
    "\n",
    "Classify the following text into one of these categories and make sure that the category name is only from the ones that I have provided. Only provide the class name as the response:\n",
    "Text: \"{text}\"\n",
    "\"\"\"\n",
    "\n",
    "# Generate prompts for rows with non-empty text\n",
    "data['prompt'] = data['text'].apply(create_prompt)\n",
    "\n",
    "# Initialize the Llama 3.1 model\n",
    "model = ChatOllama(model=\"llama3.1\")\n",
    "\n",
    "# Function to classify a single prompt using Llama 3.1\n",
    "def classify_prompt(prompt):\n",
    "    #print(\"prompt sent\")\n",
    "    # Define the template for Llama 3.1\n",
    "    template = \"\"\"Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    # Create the ChatPromptTemplate\n",
    "    chat_prompt = ChatPromptTemplate.from_template(template)\n",
    "    # Create the chain\n",
    "    chain = chat_prompt | model\n",
    "    # Invoke the model and return the classification result\n",
    "    response = chain.invoke({\"question\": prompt})\n",
    "    #print(response.content.strip())\n",
    "    return response.content.strip()  # Extract the classification result\n",
    "\n",
    "# Apply the classification function to each prompt\n",
    "data['class'] = data['prompt'].apply(classify_prompt)\n",
    "\n",
    "# Drop the 'prompt' column for clean output\n",
    "data = data[['text', 'label', 'class']]\n",
    "\n",
    "# Save the output file\n",
    "output_file_path = 'classified_results.csv'\n",
    "data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Classifications saved to {output_file_path} in the same directory as your Python file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caa7e2b-04d9-4b04-9f76-23e82f8e71e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ollama Test Environment",
   "language": "python",
   "name": "ollama_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
